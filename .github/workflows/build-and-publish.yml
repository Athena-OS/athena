name: Build & Publish Packages to R2

on:
  pull_request:
    types: [closed]
    branches:
      - main
    paths:
      - "src/**/PKGBUILD"

jobs:
  build-and-publish:
    # Only run when the PR was actually merged (not just closed/rejected)
    if: github.event.pull_request.merged == true
    runs-on: ubuntu-latest
    container:
      image: archlinux:latest
      options: --privileged

    steps:
      # ---------------------------------------------------------------
      # 1. System setup
      # ---------------------------------------------------------------
      - name: Install base dependencies
        run: |
          pacman -Syu --noconfirm
          pacman -S --noconfirm \
            base-devel \
            git \
            gnupg \
            aws-cli \
            curl \
            jq

      # ---------------------------------------------------------------
      # 2. Checkout the repository
      # ---------------------------------------------------------------
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.merge_commit_sha }}
          fetch-depth: 0

      # ---------------------------------------------------------------
      # 3. Trust the workspace directory (Git safe.directory)
      # ---------------------------------------------------------------
      - name: Trust workspace directory
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      # ---------------------------------------------------------------
      # 4. Detect which packages changed in this PR
      # ---------------------------------------------------------------
      - name: Detect changed packages
        id: changed
        run: |
          CHANGED_PKGBUILDS=$(git diff --name-only --diff-filter=ACMRT \
            HEAD^1...HEAD \
            | grep 'src/.*/PKGBUILD$' || true)

          if [ -z "$CHANGED_PKGBUILDS" ]; then
            echo "No PKGBUILD changes found. Skipping."
            echo "packages=" >> "$GITHUB_OUTPUT"
          else
            DIRS=$(echo "$CHANGED_PKGBUILDS" | xargs -I{} dirname {} | sort -u | tr '\n' ' ')
            echo "Packages to build: $DIRS"
            echo "packages=$DIRS" >> "$GITHUB_OUTPUT"
          fi

      # ---------------------------------------------------------------
      # 5. Create a non-root build user (makepkg refuses to run as root)
      # ---------------------------------------------------------------
      - name: Create build user
        if: steps.changed.outputs.packages != ''
        run: |
          useradd -m builder
          echo "builder ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
          chown -R builder:builder "$GITHUB_WORKSPACE"

      # ---------------------------------------------------------------
      # 6. Import GPG signing key into builder's keyring
      # ---------------------------------------------------------------
      - name: Import GPG key
        if: steps.changed.outputs.packages != ''
        env:
          GPG_PRIVATE_KEY: ${{ secrets.GPG_PRIVATE_KEY }}
          GPG_KEY_ID: ${{ secrets.GPG_KEY_ID }}
          GPG_PASSPHRASE: ${{ secrets.GPG_PASSPHRASE }}
        run: |
          # Import into builder's keyring (makepkg and repo-add run as builder)
          echo -n "$GPG_PRIVATE_KEY" | su builder -c \
            "gpg --batch --passphrase '$GPG_PASSPHRASE' --pinentry-mode loopback --import"

          FINGERPRINT=$(su builder -c "gpg --with-colons --fingerprint '$GPG_KEY_ID'" \
            | grep '^fpr' | head -1 | cut -d: -f10)
          echo "$FINGERPRINT:6:" | su builder -c "gpg --import-ownertrust"
          echo "GPGKEY=$GPG_KEY_ID" >> /etc/makepkg.conf

          # Configure GPG agent for non-interactive passphrase use
          su builder -c "mkdir -p ~/.gnupg && echo 'pinentry-mode loopback' >> ~/.gnupg/gpg.conf"
          su builder -c "echo 'allow-loopback-pinentry' >> ~/.gnupg/gpg-agent.conf"
          su builder -c "echo 'default-cache-ttl 86400' >> ~/.gnupg/gpg-agent.conf"

          # Pre-cache the passphrase in the agent so makepkg/repo-add are not prompted
          echo "$GPG_PASSPHRASE" | su builder -c \
            "gpg --batch --passphrase-fd 0 --pinentry-mode loopback \
             --sign --armor --output /dev/null /dev/null" 2>/dev/null || true

      # ---------------------------------------------------------------
      # 7. Build each changed package
      # ---------------------------------------------------------------
      - name: Build packages
        if: steps.changed.outputs.packages != ''
        env:
          PACKAGES: ${{ steps.changed.outputs.packages }}
        run: |
          mkdir -p /tmp/athena-repo

          for pkg_dir in $PACKAGES; do
            echo "=============================="
            echo "Building: $pkg_dir"
            echo "=============================="

            su builder -c "
              cd '$GITHUB_WORKSPACE/$pkg_dir'
              makepkg \
                --noconfirm \
                --syncdeps \
                --sign \
                --cleanbuild \
                2>&1
            "

            # Collect built packages and their signatures
            find "$GITHUB_WORKSPACE/$pkg_dir" \
              -maxdepth 1 \
              \( -name "*.pkg.tar.zst" -o -name "*.pkg.tar.zst.sig" \) \
              -exec mv {} /tmp/athena-repo/ \;
          done

          echo "Built artifacts:"
          ls -lh /tmp/athena-repo/

      # ---------------------------------------------------------------
      # 8. Download current repo DB from R2, add new packages, re-sign
      # ---------------------------------------------------------------
      - name: Update repository database
        if: steps.changed.outputs.packages != ''
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          REPO_NAME: athena
          REPO_ARCH: x86_64
        run: |
          DB_PATH="/tmp/athena-repo"
          REMOTE_PREFIX="s3://$R2_BUCKET/$REPO_NAME/$REPO_ARCH"

          # Pull down the existing database files (silently fails on first run)
          aws s3 cp "$REMOTE_PREFIX/$REPO_NAME.db.tar.gz" "$DB_PATH/$REPO_NAME.db.tar.gz" \
            --endpoint-url "$R2_ENDPOINT" || true
          aws s3 cp "$REMOTE_PREFIX/$REPO_NAME.files.tar.gz" "$DB_PATH/$REPO_NAME.files.tar.gz" \
            --endpoint-url "$R2_ENDPOINT" || true

          # Recreate the symlinks repo-add expects
          [ -f "$DB_PATH/$REPO_NAME.db.tar.gz" ] && \
            ln -sf "$REPO_NAME.db.tar.gz" "$DB_PATH/$REPO_NAME.db"
          [ -f "$DB_PATH/$REPO_NAME.files.tar.gz" ] && \
            ln -sf "$REPO_NAME.files.tar.gz" "$DB_PATH/$REPO_NAME.files"

          # Ensure builder owns the repo directory (GPG key is in builder's keyring)
          chown -R builder:builder "$DB_PATH"

          # Run repo-add as builder so it has access to the GPG key
          for pkg in "$DB_PATH"/*.pkg.tar.zst; do
            echo "Adding $(basename $pkg) to repo DB..."
            su builder -c "repo-add --sign '$DB_PATH/$REPO_NAME.db.tar.gz' '$pkg'"
          done

      # ---------------------------------------------------------------
      # 9. Upload packages + updated DB to R2, then remove old versions
      # ---------------------------------------------------------------
      - name: Upload to R2
        if: steps.changed.outputs.packages != ''
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          REPO_NAME: athena
          REPO_ARCH: x86_64
        run: |
          REMOTE_PREFIX="s3://$R2_BUCKET/$REPO_NAME/$REPO_ARCH"

          # Upload package files and their signatures
          for file in /tmp/athena-repo/*.pkg.tar.zst \
                      /tmp/athena-repo/*.pkg.tar.zst.sig; do
            [ -f "$file" ] || continue
            echo "Uploading $(basename $file)..."
            aws s3 cp "$file" "$REMOTE_PREFIX/$(basename $file)" \
              --endpoint-url "$R2_ENDPOINT"
          done

          # Upload DB archives and their signatures
          for ext in db.tar.gz db.tar.gz.sig files.tar.gz files.tar.gz.sig; do
            file="/tmp/athena-repo/$REPO_NAME.$ext"
            [ -f "$file" ] || continue
            echo "Uploading $REPO_NAME.$ext..."
            aws s3 cp "$file" "$REMOTE_PREFIX/$REPO_NAME.$ext" \
              --endpoint-url "$R2_ENDPOINT"
          done

          # Expose unversioned names (.db and .files) with their matching signatures.
          # The .sig files must correspond exactly to the content being served â€”
          # uploading .db.tar.gz.sig as .db.sig ensures pacman can verify correctly.
          echo "Uploading unversioned DB names..."
          aws s3 cp "/tmp/athena-repo/$REPO_NAME.db.tar.gz" \
            "$REMOTE_PREFIX/$REPO_NAME.db" --endpoint-url "$R2_ENDPOINT"
          aws s3 cp "/tmp/athena-repo/$REPO_NAME.db.tar.gz.sig" \
            "$REMOTE_PREFIX/$REPO_NAME.db.sig" --endpoint-url "$R2_ENDPOINT"
          aws s3 cp "/tmp/athena-repo/$REPO_NAME.files.tar.gz" \
            "$REMOTE_PREFIX/$REPO_NAME.files" --endpoint-url "$R2_ENDPOINT"
          aws s3 cp "/tmp/athena-repo/$REPO_NAME.files.tar.gz.sig" \
            "$REMOTE_PREFIX/$REPO_NAME.files.sig" --endpoint-url "$R2_ENDPOINT"

          # Remove old package versions from R2.
          # Done AFTER uploading the new version so the repo is never left empty.
          for pkg in /tmp/athena-repo/*.pkg.tar.zst; do
            [ -f "$pkg" ] || continue
            PKGNAME=$(basename "$pkg" | rev | cut -d- -f4- | rev)
            NEWFILE=$(basename "$pkg")

            aws s3 ls "$REMOTE_PREFIX/" --endpoint-url "$R2_ENDPOINT" \
              | awk '{print $4}' \
              | grep "^${PKGNAME}-" \
              | grep '\.pkg\.tar\.zst$' \
              | grep -v "^${NEWFILE}$" \
              | while read -r old_file; do
                  echo "Removing old: $old_file"
                  aws s3 rm "$REMOTE_PREFIX/$old_file" --endpoint-url "$R2_ENDPOINT"
                  aws s3 rm "$REMOTE_PREFIX/$old_file.sig" --endpoint-url "$R2_ENDPOINT" || true
                done
          done

      # ---------------------------------------------------------------
      # 10. Purge Cloudflare CDN cache for updated files
      # ---------------------------------------------------------------
      - name: Purge edge cache
        if: steps.changed.outputs.packages != ''
        env:
          CF_ZONE_ID: ${{ secrets.CF_ZONE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
          REPO_NAME: athena
        run: |
          REPO_BASE_URL="https://hub.athenaos.org/$REPO_NAME/x86_64"
          URLS=()

          # Add every uploaded package to the purge list
          for f in /tmp/athena-repo/*.pkg.tar.zst \
                   /tmp/athena-repo/*.pkg.tar.zst.sig; do
            [ -f "$f" ] || continue
            URLS+=("$REPO_BASE_URL/$(basename $f)")
          done

          # Always purge all DB-related URLs including unversioned names
          for name in db db.sig db.tar.gz db.tar.gz.sig \
                      files files.sig files.tar.gz files.tar.gz.sig; do
            URLS+=("$REPO_BASE_URL/$REPO_NAME.$name")
          done

          # Cloudflare allows up to 30 URLs per call; build JSON and POST
          JSON_URLS=$(printf '%s\n' "${URLS[@]}" | jq -R . | jq -s .)
          curl -s -X POST \
            "https://api.cloudflare.com/client/v4/zones/$CF_ZONE_ID/purge_cache" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "{\"files\": $JSON_URLS}" | jq .
